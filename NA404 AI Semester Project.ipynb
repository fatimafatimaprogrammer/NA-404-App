{"cells":[{"cell_type":"markdown","source":["## **NA-404 AI SEMESTER PROJECT**\n","\n","*GROUP MEMBERS*\n","1.   Kainat Sajid   - 289051\n","2.   Saleha Farooqi - 285808\n","3.   Urooj Fatima   - 289365\n","4.   Zainab Ghauri  - 296357\n","\n"],"metadata":{"id":"qqxEVyi_V7Mi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2N-Ib-lw-1ci"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","%cd gdrive/My Drive/AI_Project_ESP\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvMg9VR9BZiM"},"outputs":[],"source":["# import tweepy\n","\n","from tweepy import *\n","import pandas as pd\n","import csv\n","import re \n","import string\n","from sklearn import preprocessing as p\n","#import preprocessor as p\n"," \n","import tweepy as tw\n","# your Twitter API key and API secret\n","my_api_key = \"lZjEaA01Yc0bmAMVv2GeZyIlK\"\n","my_api_secret = \"FtYliKfYoxGrvki50rwJguK3DA1V32P4MRRgTHOTsU3ayPazJX\"\n","# authenticate\n","auth = tw.OAuthHandler(my_api_key, my_api_secret)\n","api = tw.API(auth, wait_on_rate_limit=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xRruxlA-3zv"},"outputs":[],"source":["'''\n","6.Shahbaz Sharif\n","7.Reham Khan\n","8.Corruption\n","9.PMLN\n","10.PTI\n","\n","11.Hamid Mir\n","12.Ary News\n","13.high security for ik\n","14.lotai\n","15.ab sirf khooni inklab (urdu)\n","'''\n","search_query = \"#absirfkhooniinklab -filter:retweets\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":603,"status":"ok","timestamp":1652942587151,"user":{"displayName":"fatima raza","userId":"05068512054796093753"},"user_tz":-300},"id":"xspI4_bs-mNY","outputId":"88e40b93-a01a-4c30-cc56-056b992c4ee4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total Tweets fetched: 0\n"]}],"source":["tweets = tw.Cursor(api.search,\n","              q=search_query,\n","              lang=\"en\",\n","              ).items(500)\n","# store the API responses in a list\n","tweets_copy = []\n","for tweet in tweets:\n","    tweets_copy.append(tweet)\n","    \n","print(\"Total Tweets fetched:\", len(tweets_copy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"utmllXDZ-q9q"},"outputs":[],"source":["import pandas as pd\n","# intialize the dataframe\n","tweets_df = pd.DataFrame()\n","# populate the dataframe\n","import csv\n","csvFile = open('14.csv', 'a')\n","csvWriter = csv.writer(csvFile)\n"," \n","for tweet in tweets_copy:\n","    hashtags = []\n","    try:\n","        for hashtag in tweet.entities[\"hashtags\"]:\n","            hashtags.append(hashtag[\"text\"])\n","        text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n","    except:\n","        pass\n","    tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name, \n","                                               'user_location': tweet.user.location,\\\n","                                               'user_description': tweet.user.description,\n","                                               'user_verified': tweet.user.verified,\n","                                               'date': tweet.created_at,\n","                                               'text': text, \n","                                               'hashtags': [hashtags if hashtags else None],\n","                                               'source': tweet.source}))\n","    csvWriter.writerow([tweet.user.name.encode('utf-8'), tweet.user.location.encode('utf-8'),tweet.user.description.encode('utf-8'), tweet.user.verified ,  tweet.created_at , text.encode('utf-8')])\n","    tweets_df = tweets_df.reset_index(drop=True)\n","# show the dataframe\n","tweets_df.head()"]},{"cell_type":"markdown","source":["**Data Read**"],"metadata":{"id":"8Y9JByNnNlqf"}},{"cell_type":"code","source":["from __future__ import print_function\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm, datasets\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn import svm, datasets\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","\n","import pandas as pd\n","\n","pd.__version__\n","#Setting names for the csv header\n","headernames = ['Name', 'Location', 'Bio', 'Verified/Not_verifies', 'Date/Time', 'Tweets','Party','Response']\n","\n","#opening the csv file\n","dataset = pd.read_csv(\"/content/gdrive/MyDrive/AI project/TWEETS.csv\", names = headernames)\n","#dataset = pd.read_excel (r'TrainingSet.xlsx')\n","dataset.head()\n","#print(dataset.head())\n","#Seperating the input features and output labels\n","X = dataset.iloc[:, 5:6].values\n","y = dataset.iloc[:, 6].values\n","y1 = dataset.iloc[:, 7].values\n","#print(X)\n","#print(y)\n","#print(y1)"],"metadata":{"id":"1K5f5xldNs58"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Data Preprocessing**"],"metadata":{"id":"uFYW6TlsNuDu"}},{"cell_type":"code","source":["dataset.dtypes\n","dataset.describe()"],"metadata":{"id":"PKONW_NDb5F3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import print_function\n","\n","import pandas as pd\n","pd.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"rfpudYTixNMl","executionInfo":{"status":"ok","timestamp":1653479059007,"user_tz":-300,"elapsed":506,"user":{"displayName":"Kaynat Sajid","userId":"08851532999694769461"}},"outputId":"5593514c-308c-4978-83e5-38b5c8cf4b84"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.3.5'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["print(dataset.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-XsTLfyOxNVq","executionInfo":{"status":"ok","timestamp":1653479061036,"user_tz":-300,"elapsed":7,"user":{"displayName":"Kaynat Sajid","userId":"08851532999694769461"}},"outputId":"d660c239-e07b-4cab-9549-17bf7c06bd18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2778, 8)\n"]}]},{"cell_type":"code","source":["''' we use Beautiful soap4 for removing HTML tags\n","pip install beautifulsoup4\n","'''\n","'''\n","Punctuation and numbers also don’t help in deciding the sentiment of a review.\n","We will remove them using the package re (regular expression). \n","\n","Removing punctuation is a common preprocessing step in many data analysis and machine learning tasks.\n","\n","if you’re working on user-generated text data such as social media posts, you’d encounter too much punctuation in the sentences, which may not be useful for the task at hand, and so removing all of them becomes an essential pre-processing task.\n","\n","'''\n","import numpy as np\n","import pandas as pd\n","import re\n","import nltk\n","import spacy\n","import string\n","pd.options.mode.chained_assignment = None\n","\n","#dataset = pd.read_csv(\"../input/customer-support-on-twitter/twcs/twcs.csv\", nrows=5000)\n","df = dataset[[\"Tweets\"]]\n","df[\"Tweets\"] = df[\"Tweets\"].astype(str)\n","dataset.head()\n"],"metadata":{"id":"NfFPlJXmxNhT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Lower Casing**\n","\n","Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way.\n","\n","This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.\n","\n","This may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)\n","\n","By default, lower casing is done my most of the modern day vecotirzers and tokenizers like sklearn TfidfVectorizer and Keras Tokenizer. So we need to set them to false as needed depending on our use case.\n"],"metadata":{"id":"t-oRIKag2eYb"}},{"cell_type":"code","source":["df[\"Tweets_lower\"] = df[\"Tweets\"].str.lower()\n","df.head()"],"metadata":{"id":"jFP66xGPxNqR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Removal of Punctuations**\n","\n","One another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n","\n","We also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the string.punctuation in python contains the following punctuation symbols\n","\n","!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n","\n","We can add or remove more punctuations as per our need.\n"],"metadata":{"id":"UEJu5Mqn2zE9"}},{"cell_type":"code","source":["# drop the new column created in last cell\n","'''df.drop([\"Tweets_lower\"], axis=1, inplace=True)'''\n","\n","PUNCT_TO_REMOVE = string.punctuation\n","def remove_punctuation(Tweets):\n","    \"\"\"custom function to remove the punctuation\"\"\"\n","    return Tweets.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n","\n","df[\"Tweets_wo_punct\"] = df[\"Tweets\"].apply(lambda Tweets: remove_punctuation(Tweets))\n","df.head()"],"metadata":{"id":"Iy1N4Xd42l8m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Removal of stopwords**\n","\n","Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\n","\n","These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen below.\n"],"metadata":{"id":"ZgRPZXmQ3nwP"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\", \".join(stopwords.words('english'))"],"metadata":{"id":"ulDc9iG42l_i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["STOPWORDS = set(stopwords.words('english'))\n","def remove_stopwords(Tweets):\n","    \"\"\"custom function to remove the stopwords\"\"\"\n","    return \" \".join([word for word in str(Tweets).split() if word not in STOPWORDS])\n","\n","df[\"Tweets_wo_stop\"] = df[\"Tweets_wo_punct\"].apply(lambda Tweets: remove_stopwords(Tweets))\n","df.head()"],"metadata":{"id":"mOVOTSiY2mC5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Removal of Frequent words and Removal of Rare word**\n","\n","In the previos preprocessing step, we removed the stopwords based on language information. But say, if we have a domain specific corpus, we might also have some frequent words which are of not so much importance to us.\n","\n","So this step is to remove the frequent words in the given corpus. If we use something like tfidf, this is automatically taken care of.\n","\n","Let us get the most common words adn then remove them in the next step\n"],"metadata":{"id":"upau5fKj4W-t"}},{"cell_type":"code","source":["from collections import Counter\n","cnt = Counter()\n","for Tweets in df[\"Tweets_wo_stop\"].values:\n","    for word in Tweets.split():\n","        cnt[word] += 1\n","        \n","cnt.most_common(10)"],"metadata":{"id":"BxQcUder2mMN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n","def remove_freqwords(Tweets):\n","    \"\"\"custom function to remove the frequent words\"\"\"\n","    return \" \".join([word for word in str(Tweets).split() if word not in FREQWORDS])\n","\n","df[\"Tweets_wo_stopfreq\"] = df[\"Tweets_wo_stop\"].apply(lambda Tweets: remove_freqwords(Tweets))\n","df.head()"],"metadata":{"id":"-0xfTTpC4Zty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop the two columns which are no more needed \n","#df.drop([\"Tweets_wo_punct\", \"Tweets_wo_stop\"], axis=1, inplace=True)\n","\n","n_rare_words = 10\n","RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n","def remove_rarewords(Tweets):\n","    \"\"\"custom function to remove the rare words\"\"\"\n","    return \" \".join([word for word in str(Tweets).split() if word not in RAREWORDS])\n","\n","df[\"Tweets_wo_stopfreqrare\"] = df[\"Tweets_wo_stopfreq\"].apply(lambda Tweets: remove_rarewords(Tweets))\n","df.head()"],"metadata":{"id":"iRPR-pZV7KLD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Removal of URLs**\n","\n","Next preprocessing step is to remove any URLs present in the data. For example, if we are doing a twitter analysis, then there is a good chance that the tweet will have some URL in it. Probably we might need to remove them for our further analysis.\n","\n","We can use the below code snippet to do that.\n"],"metadata":{"id":"OFhooF5W5RbV"}},{"cell_type":"code","source":["def remove_urls(Tweets):\n","    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","    return url_pattern.sub(r'', Tweets)\n","df[\"Tweets_wo_url\"] = df[\"Tweets_wo_stopfreqrare\"].apply(lambda Tweets: remove_urls(Tweets))\n","df.head()"],"metadata":{"id":"68m2QaNb4ZwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","dataset.head()"],"metadata":{"id":"DUYsJCn2O4dO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"LgC9lhTVO91Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Converting input to vector**"],"metadata":{"id":"BNbHjsCNWTAV"}},{"cell_type":"code","source":["from sklearn import preprocessing\n","import pandas as pd \n","import numpy as np\n","from sklearn.svm import SVC\n","from sklearn.feature_extraction.text import CountVectorizer\n","#print(df.columns[3])\n","data = df[df.columns[3]]\n","\n","train_x = data.to_numpy().flatten()\n","#print('train_x',train_x)\n","vectorizer = CountVectorizer(binary=True)\n","vector = vectorizer.fit_transform(train_x)\n","#print(vector)"],"metadata":{"id":"-YorXC0m4Zzz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**converting output (parties) into numerical form**"],"metadata":{"id":"98x6XoBZWYZO"}},{"cell_type":"code","source":["#converting text labels to numeric form\n","labels, unique = pd.factorize(y)\n","labels1 ,unique = pd.factorize(y1)\n","'''\n","parties\n","0 = PMLN\n","1 = N\n","2 = PTI\n","3 = PPP\n","\n","response\n","Neq = 0\n","P = 1\n","N = 2\n","'''\n","#print('labels: ',labels)\n","print('labels1: ',labels1)\n","#splitting data in test and train segments\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(vector, labels, test_size = 0.20)\n","X_train1, X_test1, y_train1, y_test1 = train_test_split(vector, labels1, test_size = 0.20)\n","#print('x_train : ',X_train)\n","print('x_test1: ',X_test)\n","print('y_test1: ',y_test)\n","'''\n"," ytest1_predicted [2 0 3 2 2 0 2 2 2 1 2 2 2 1 0 0 0 2 0 0 1 2 1 1 2 2 0 2 0 1 1 0 0 2 2 2 3\n"," 1 2 1 2 2 0 0 2 0 2 2 0 2 2 2 2 1 1 2 1 0 2 2 0 2 0 1 0 0 0 2 2 2 2 1 2 2\n"," 2 2 2 2 2 2 0 0 2 1 2 2 2 1 2 2 2 2 2 0 1 1 0 1 0 0 0]\n","\n"," y_test1_original[2 2 1 0 2 2 2 0 1 0 1 1 2 0 1 2 2 1 0 1 2 0 0 2 0 2 2 2 2 2 1 2 2 0 2 0 1\n"," 2 1 1 1 0 2 2 0 1 2 2 1 2 2 1 0 2 0 0 0 2 2 1 2 0 2 2 0 1 2 2 2 0 2 0 0 2\n"," 0 2 2 2 1 1 0 2 0 2 1 1 1 1 1 0 1 2 0 3 2 1 0 1 2 1 2]\n"," '''"],"metadata":{"id":"4_Jm-b0Vc_bn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"990kn2BBmxXT"}},{"cell_type":"code","source":["#Importing required library\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","\n","# multi-class classification with Keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.utils import np_utils\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.pipeline import Pipeline\n","\n","# define baseline model\n","def baseline_model():\n","\t# create model\n","\tmodel = Sequential()\n","\tmodel.add(Dense(8, input_dim=1, activation='relu'))\n","\tmodel.add(Dense(4, activation='softmax'))\n","\t# Compile model\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\treturn model\n"," \n","estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n","kfold = KFold(n_splits=10, shuffle=True)\n","results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n","print('============================Results with 10 splits of kfold============================\\n',results)\n","print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"],"metadata":{"id":"Udk-5K4matHk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **SVMS Classification**"],"metadata":{"id":"dvNuuYoqN5tp"}},{"cell_type":"markdown","source":["**Prediction for output1(Party)**"],"metadata":{"id":"BGnbmGloXnIN"}},{"cell_type":"code","source":["rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(X_train, y_train)\n","poly = svm.SVC(kernel='poly', degree=3, C=1).fit(X_train, y_train)\n","lin = svm.SVC(kernel = 'linear').fit(X_train,y_train)\n","\n","poly_pred = poly.predict(X_test)\n","rbf_pred = rbf.predict(X_test)\n","lin_pred = lin.predict(X_test)\n","\n","print('rbf_pred', rbf_pred)\n","\n","#=====================================================\n","#   Accuracy through RBF Kernel\n","#=====================================================\n","rbf_pred = rbf.predict(X_train)\n","rbf_accuracy = accuracy_score(y_train, rbf_pred)\n","rbf_f1 = f1_score(y_train, rbf_pred, average='weighted')\n","print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n","print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))\n","\n","#=====================================================\n","#   Accuracy through linear Kernel\n","#=====================================================\n","print('\\nlin_pred', lin_pred)\n","lin_pred = lin.predict(X_train)\n","lin_accuracy = metrics.accuracy_score(y_train, lin_pred)\n","lin_f1 = f1_score(y_train, lin_pred, average='weighted')\n","print('Accuracy (lin Kernel): ', \"%.2f\" % (lin_accuracy*100))\n","print('F1 (lin Kernel): ', \"%.2f\" % (lin_f1*100))\n","\n","#=====================================================\n","#   Accuracy through poly Kernel\n","#=====================================================\n","print('\\npoly_pred' , poly_pred)\n","poly_pred = poly.predict(X_train)\n","poly_accuracy = accuracy_score(y_train,poly_pred)\n","poly_f1 = f1_score(y_train,poly_pred, average='weighted')\n","print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n","print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n"],"metadata":{"id":"MG6qEDnMOCMq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Prediction for output2(response)**"],"metadata":{"id":"-7uhq7g8XtE7"}},{"cell_type":"code","source":["rbf1 = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(X_train1, y_train1)\n","poly1 = svm.SVC(kernel='poly', degree=3, C=1).fit(X_train1, y_train1)\n","lin1 = svm.SVC(kernel = 'linear').fit(X_train1,y_train1)\n","\n","poly_pred1 = poly.predict(X_test1)\n","rbf_pred1 = rbf.predict(X_test1)\n","lin_pred1 = lin.predict(X_test1)\n","\n","print('rbf_pred', rbf_pred1)\n","\n","#=====================================================\n","#   Accuracy through RBF Kernel\n","#=====================================================\n","rbf_pred1 = rbf.predict(X_train1)\n","rbf_accuracy1 = accuracy_score(y_train1, rbf_pred1)\n","rbf_f1 = f1_score(y_train1, rbf_pred1, average='weighted')\n","print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n","print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))\n","\n","#=====================================================\n","#   Accuracy through linear Kernel\n","#=====================================================\n","print('\\nlin_pred', lin_pred1)\n","lin_pred1 = lin.predict(X_train1)\n","lin_accuracy1 = metrics.accuracy_score(y_train1, lin_pred1)\n","lin_f1 = f1_score(y_train1, lin_pred1, average='weighted')\n","print('Accuracy (lin Kernel): ', \"%.2f\" % (lin_accuracy*100))\n","print('F1 (lin Kernel): ', \"%.2f\" % (lin_f1*100))\n","\n","#=====================================================\n","#   Accuracy through poly Kernel\n","#=====================================================\n","print('\\npoly_pred' , poly_pred1)\n","poly_pred1 = poly.predict(X_train1)\n","poly_accuracy1 = accuracy_score(y_train1,poly_pred1)\n","poly_f1 = f1_score(y_train1,poly_pred1, average='weighted')\n","print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n","print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n"],"metadata":{"id":"zOjcUqgRXkad"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **DECISION TREES IN PYTHON**\n"],"metadata":{"id":"tNMakKFnqyiM"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n","from sklearn.model_selection import train_test_split # Import train_test_split function\n","from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n"],"metadata":{"id":"GM9swbrCrkyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#converting text labels to numeric form\n","labels, unique = pd.factorize(y)\n","labels1 ,unique = pd.factorize(y1)\n"],"metadata":{"id":"FIBQu5eaq-uH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split dataset into training set and test set\n","X_train, X_test, y_train, y_test = train_test_split(vector, labels, test_size=0.3, random_state=1) # 70% training and 30% test\n","X_train1, X_test1, y_train1, y_test1 = train_test_split(vector, labels1, test_size=0.3, random_state=1)\n"],"metadata":{"id":"plyMjQi1rBBp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=====================================================\n","#  Decision Trees (Response)\n","#=====================================================\n","\n","# Create Decision Tree classifer object\n","clf = DecisionTreeClassifier()\n","\n","# Train Decision Tree Classifer\n","clf = clf.fit(X_train,y_train)\n","\n","#Predict the response for test dataset\n","y_pred = clf.predict(X_test)"],"metadata":{"id":"UJlm9NJrrIYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=====================================================\n","#   Accuracy through Decision Trees (Response)\n","#=====================================================\n","print('\\nY_pred', y_pred)\n","y_pred = clf.predict(X_test)\n","decision_accuracy = metrics.accuracy_score(y_test, y_pred)\n","decision_f1 = f1_score(y_test, y_pred, average='weighted')\n","print('Accuracy : ', \"%.2f\" % (decision_accuracy*100))\n","print('F1 : ', \"%.2f\" % (decision_f1*100))\n"],"metadata":{"id":"jtwBGaVpT4oI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=====================================================\n","#  Decision Trees (Party)\n","#=====================================================\n","\n","# Create Decision Tree classifer object\n","clf = DecisionTreeClassifier()\n","\n","# Train Decision Tree Classifer\n","clf = clf.fit(X_train1,y_train1)\n","\n","#Predict the response for test dataset\n","y_pred = clf.predict(X_test1)"],"metadata":{"id":"ir27F0AzrKIl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#=====================================================\n","#   Accuracy through Decision Trees (Party)\n","#=====================================================\n","print('\\nY_pred', y_pred)\n","y_pred = clf.predict(X_test1)\n","decision_accuracy = metrics.accuracy_score(y_test1, y_pred)\n","decision_f1 = f1_score(y_test1, y_pred, average='weighted')\n","print('Accuracy : ', \"%.2f\" % (decision_accuracy*100))\n","print('F1 : ', \"%.2f\" % (decision_f1*100))\n"],"metadata":{"id":"A5WQLi0PrhjK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["report = metrics.classification_report(y_test1,y_pred)\n","print(report)"],"metadata":{"id":"XEV1opDUrnId"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **RANDOM FOREST CLASSIFIER**"],"metadata":{"id":"PQ5yofyzwRQ0"}},{"cell_type":"code","source":["#Import Random Forest Model\n","from sklearn.ensemble import RandomForestClassifier\n","#Import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics"],"metadata":{"id":"DXnjeqSOwVFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#converting text labels to numeric form\n","labels, unique = pd.factorize(y)\n","labels1 ,unique = pd.factorize(y1)\n"],"metadata":{"id":"G381vfalv5I5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**FOR RESPONSE :**"],"metadata":{"id":"4a2XPO8-UQ1m"}},{"cell_type":"code","source":["\n","#Create a Gaussian Classifier\n","clf=RandomForestClassifier(n_estimators=100)\n","\n","#Train the model using the training sets y_pred=clf.predict(X_test)\n","clf.fit(X_train,y_train)\n","\n","y_pred=clf.predict(X_test)\n"],"metadata":{"id":"qWQDJwwgv-ZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_pred)\n","# Model Accuracy, how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"],"metadata":{"id":"m0XyrVMxv-71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["report = metrics.classification_report(y_test,y_pred)\n","print(report)"],"metadata":{"id":"LzUCjiy9UgIv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**FOR PARTY**\n"],"metadata":{"id":"FxkjUX-dJK0Y"}},{"cell_type":"code","source":["\n","#Create a Gaussian Classifier\n","clf=RandomForestClassifier(n_estimators=100)\n","\n","#Train the model using the training sets y_pred=clf.predict(X_test)\n","clf.fit(X_train1,y_train1)\n","\n","y_pred=clf.predict(X_test1)\n"],"metadata":{"id":"jQCsRi4bP6ST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model Accuracy, how often is the classifier correct?\n","print(y_pred)\n","print(\"Accuracy:\",metrics.accuracy_score(y_test1, y_pred))"],"metadata":{"id":"GuoZ4T9XQfMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["report = metrics.classification_report(y_test1,y_pred)\n","print(report)"],"metadata":{"id":"XjTCStfxUr2G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **NAIVE BAYES**"],"metadata":{"id":"TJpP8rz5UwzI"}},{"cell_type":"code","source":["\n","\"\"\"   NAIVE BAYES  START  \"\"\"\n","\n","import sklearn.metrics as metrics\n","# Split into training and testing data\n","x = data['tweet'].astype(str)\n","y = data['response'].astype(str)\n","z = data['party'].astype(str)\n","\n","x, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=42)\n","\n","vec = CountVectorizer(stop_words='english')\n","x_train = vec.fit_transform(x).toarray()\n","x_test = vec.transform(x_test).toarray()\n","\n","model = MultinomialNB()\n","model.fit(x_train, y_train)\n","model.score(x_test, y_test)\n","\n","\n","z_train, z_test = train_test_split(z,test_size=0.25, random_state=42)\n","\n","\n","model1 = MultinomialNB()\n","model1.fit(x_train, z_train)\n","model1.score(x_test, z_test)\n","\n","z_pred = model1.predict(x_train)\n","\n","report = metrics.classification_report(z_train, z_pred)\n","print(report)"],"metadata":{"id":"bPFhPGeiU4cM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"_____________________________________________________________________________\")\n","tweet = input(\"Please enter the tweet you want to analyse: \")\n","tweet = preprocess_tweet(tweet)\n","sentiment = model.predict(vec.transform([tweet]))\n","party = model1.predict(vec.transform([tweet]))\n","print(\"This tweet is:\", end =\" \")\n","print(sentiment)\n","print(\"This tweet is about:\", end =\" \")\n","print(party)\n","print (\"_____________________________________________________________________________\")\n","\n","\n","\"\"\"   NAIVE BAYES END \"\"\"\n","\n"],"metadata":{"id":"jEIgnRwZVAp5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **NEURAL NETWORKS**"],"metadata":{"id":"xraNoH4UVBVk"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn import preprocessing\n","from collections import Counter\n","from nltk.corpus import stopwords\n","import string\n","import re\n","\n","data = pd.read_csv('TWEETS.csv')\n","cnt = Counter()\n","\n","def remove_punctuation(Tweets):\n","    \"\"\"custom function to remove the punctuation\"\"\"\n","    PUNCT_TO_REMOVE = string.punctuation\n","    return Tweets.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n","def remove_stopwords(Tweets):\n","    \"\"\"custom function to remove the stopwords\"\"\"\n","    STOPWORDS = set(stopwords.words('english'))\n","    return \" \".join([word for word in str(Tweets).split() if word not in STOPWORDS])\n","def remove_freqwords(Tweets):\n","    \"\"\"custom function to remove the frequent words\"\"\"\n","    cnt.most_common(10)\n","    FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n","    return \" \".join([word for word in str(Tweets).split() if word not in FREQWORDS])\n","def remove_rarewords(Tweets):\n","    \"\"\"custom function to remove the rare words\"\"\"\n","    n_rare_words = 10\n","    RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n","    return \" \".join([word for word in str(Tweets).split() if word not in RAREWORDS])\n","def remove_urls(Tweets):\n","    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","    return url_pattern.sub(r'', Tweets)\n","\n","def preprocess_data(data):\n","    data = data.drop('name',axis=1)\n","    data = data.drop('location',axis=1)\n","    data = data.drop('bio',axis=1)\n","    data = data.drop('verified',axis=1)\n","    data = data.drop('date',axis=1)\n","    \n","    data['tweet'] = data['tweet'].str.strip().str.lower()\n","    data[\"Tweets_wo_punct\"] = data[\"tweet\"].apply(lambda Tweets: remove_punctuation(Tweets))\n","    \", \".join(stopwords.words('english'))\n","    data[\"Tweets_wo_stop\"] = data[\"Tweets_wo_punct\"].apply(lambda Tweets: remove_stopwords(Tweets))\n","\n","    for Tweets in data[\"Tweets_wo_stop\"].values:\n","        for word in Tweets.split():\n","            cnt[word] += 1\n","\n","    data[\"Tweets_wo_stopfreq\"] = data[\"Tweets_wo_stop\"].apply(lambda Tweets: remove_freqwords(Tweets))\n","    data[\"Tweets_wo_stopfreqrare\"] = data[\"Tweets_wo_stopfreq\"].apply(lambda Tweets: remove_rarewords(Tweets))\n","    data[\"Tweets_wo_url\"] = data[\"Tweets_wo_stopfreqrare\"].apply(lambda Tweets: remove_urls(Tweets))\n","\n","    return data\n","\n","data = data.astype(str)\n","data = preprocess_data(data)\n"],"metadata":{"id":"VmowLkO_VE1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_tweet(data):\n","    data = data.strip().lower()\n","    data = remove_punctuation(data)\n","    \", \".join(stopwords.words('english'))\n","    data = remove_stopwords(data)\n","    \n","    for word in data.split():\n","        cnt[word] += 1\n","\n","    data = remove_freqwords(data)\n","    data = remove_rarewords(data)\n","    data = remove_urls(data)\n","\n","    return data"],"metadata":{"id":"04xjKoXVVIcS"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"NA404 AI Semester Project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}